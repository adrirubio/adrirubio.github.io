<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adrian Rubio</title>
    <description>This is my blog where I share my coding journey, my experiences, challenges, and discoveries in the ever-evolving world of programming.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 08 Jun 2025 17:00:00 +0200</pubDate>
    <lastBuildDate>Sun, 08 Jun 2025 17:00:00 +0200</lastBuildDate>
    <generator>Jekyll v4.3.2</generator><item>
        <title>Starting Architext-AI</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! In this post, I wrapped up the AI PDF Reader by releasing a Windows version and started my next project — an AI writing assistant called Architext-AI.&lt;/p&gt;

&lt;p&gt;I started the week by creating a demo video for the AI PDF Reader, which is now embedded in  &lt;a href=&quot;https://adrianrubio.org/blog/my-ai-pdf-reader-how-and-why-I-build-it/&quot;&gt;last week’s blog post.&lt;/a&gt; I’m really happy with how the final version turned out. To wrap up the project, I released a Windows version and improved the README.
Hopefully, the AI PDF Reader can gain some traction, as my dad and I will be sharing it across various social media platforms. Our goal is to reach 150 stars on GitHub.&lt;/p&gt;

&lt;p&gt;Next, I began brainstorming ideas for my next project. With some help from my dad, we came up with a simple Python program: using a predefined command, you can open a small window where write your message, and the AI will enhance it for you—improving clarity and overall quality. I think it would be something everyone would use.&lt;/p&gt;

&lt;p&gt;Yesterday, I got started on the project. I planned out the structure and chose &lt;a href=&quot;https://docs.python.org/3/library/tkinter.html&quot;&gt;Tkinter&lt;/a&gt; for the GUI and the &lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;OpenAI SDK&lt;/a&gt; for the AI capabilites. I set up all the necessary files, and now I’m focusing on building the Python GUI. Hopefully, by next week, I’ll have something that looks good and works smoothly.&lt;/p&gt;

&lt;p&gt;This post documents the state of Architext-AI as of June 8, 2025&lt;br /&gt;
To see the current state of the pdf reader visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/architext-ai&quot;&gt;architext-ai github&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! In this post, I wrapped up the AI PDF Reader by releasing a Windows version and started my next project — an AI writing assistant called Architext-AI.&lt;/p&gt;
</description><pubDate>Sun, 08 Jun 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/starting-architext-ai/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/starting-architext-ai/</guid></item><item>
        <title>My AI PDF Reader: The Story of How (and Why) I Built It</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! In this post I&apos;m going to tell you the whole story of my AI PDF reader – like, how I built it and the reasons why. This project was a pretty wild ride with some tough challenges and fun wins, and seeing it finally work just how I wanted is awesome!&lt;/p&gt;

&lt;p&gt;I’m a huge &lt;a href=&quot;https://karpathy.ai/&quot;&gt;Andrej Karpathy&lt;/a&gt; fan (the guy who invented the term &lt;em&gt;vibe coding&lt;/em&gt;) and pretty much watch all his videos. One time, while I was watching his “How I use LLMs” video, he said that he wished there was a tool to make reading PDFs way easier. That just clicked for me! I read a lot of AI papers (about how different AI systems are built), and I often get stuck on tricky parts. The idea of highlighting text and getting help right away to understand it? That sounded awesome!&lt;/p&gt;

&lt;p&gt;He specifically imagined a tool where you could highlight text in the PDF and get AI explanations. He actually talks about the idea &lt;a href=&quot;https://www.youtube.com/watch?v=EWvNQjAaOHw&amp;amp;t=3507s&quot;&gt;right here!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, I decided to give it a shot!&lt;/p&gt;

&lt;p&gt;This was my very first desktop app using &lt;em&gt;Electron&lt;/em&gt;, so for some of the harder parts, I &lt;strong&gt;vibe coded&lt;/strong&gt; it – you know, working with an AI (Claude 3.7 via &lt;a href=&quot;https://www.cursor.com/&quot;&gt;Cursor&lt;/a&gt;) helping me along. I definitely used this chance to understand these frameworks better, and now they don’t seem as confusing as before! I also learned that &lt;em&gt;vibe coding&lt;/em&gt; isn’t as easy as it looks. You have to explain everything super clearly to the AI, and sometimes it would just get stuck, meaning I’d have to dive in and figure things out. Getting the UI to work was awesome! But sometimes trying to fix one tiny bug would just make a bunch of new ones pop up – so frustrating! Still, I learned a ton about &lt;em&gt;Electron,&lt;/em&gt; &lt;em&gt;npm&lt;/em&gt; and &lt;em&gt;JavaScript&lt;/em&gt;. I even got used to using Git branches, which was super helpful for trying new stuff without breaking everything. And when I had a stable version, I learned how to make GitHub Releases so people could actually download and use the app! Looking back it was a really great experience and I learned so much!&lt;/p&gt;

&lt;div style=&quot;text-align: center; margin: 2em 0;&quot;&gt;
  &lt;video controls=&quot;&quot; width=&quot;720&quot; style=&quot;max-width: 100%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);&quot;&gt;
    &lt;source src=&quot;https://github.com/adrirubio/ai-pdf-reader-demo/raw/main/demo-ai-pdf-reader.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Your browser does not support the video tag.
  &lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;I really hope this can be a useful tool for students, researchers, or anyone who wants to read PDFs more efficiently.
You can find the project, download the app, and see all the code on my GitHub page: &lt;a href=&quot;https://github.com/adrirubio/ai-pdf-reader&quot;&gt;AI PDF reader&lt;/a&gt;&lt;br /&gt;
If you think it’s cool or useful, starring the project on GitHub would mean the world to me! It helps other people find it and really motivates me to keep building new things. I’d also love to hear what you think or if you have any ideas!&lt;/p&gt;

&lt;p&gt;Thanks for reading and checking out my project!&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! In this post I&apos;m going to tell you the whole story of my AI PDF reader – like, how I built it and the reasons why. This project was a pretty wild ride with some tough challenges and fun wins, and seeing it finally work just how I wanted is awesome!&lt;/p&gt;
</description><pubDate>Sat, 31 May 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/my-ai-pdf-reader-how-and-why-I-build-it/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/my-ai-pdf-reader-how-and-why-I-build-it/</guid></item><item>
        <title>Improving the AI PDF Reader</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! Over the past two weeks, I’ve been working hard on the AI PDF Reader. I’ve improved the UI, integrated the AI API, and am now implementing a database to support the reader.&lt;/p&gt;

&lt;p&gt;I haven’t posted in the past two weeks as I’ve been fully focused on developing the AI PDF Reader. Now that I’ve found a moment, here’s a quick recap of what I’ve been working on and what to expect moving forward.&lt;/p&gt;

&lt;p&gt;I started the week by addressing several UI issues to make the interface cleaner and more user-friendly.
I removed the “Simple,” “Like I’m Five,” and “Technical” modes, keeping only the “Custom” option to streamline the experience. I also enlarged the chat area and slightly repositioned the PDF viewer to the left for better layout balance. Instead of dumping the entire highlighted text into the chat, it now generates a compact link or button that takes you directly to the relevant section in the PDF—making navigation much smoother. Additionally, when starting a new chat, it can now retain full context from the entire PDF.&lt;/p&gt;

&lt;p&gt;After that, I began integrating the API.
The app now supports OpenAI API keys, correctly detects them, and uses them for generating responses.
Here is a quick look at what’s changed from where we left off to this week:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/shipwrecked/pdf-reader-home.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt; &lt;img src=&quot;/assets/img/shipwrecked/pdf-reader-home2.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Also added:
&lt;img src=&quot;/assets/img/shipwrecked/full-context.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt; &lt;img src=&quot;/assets/img/shipwrecked/chat-with-full-context.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Yesterday, I joined the Shipwrecked kickoff call and got to see what other teenagers are building. It was exciting to hear from other builders, share early progress, and get inspired by a wide range of creative ideas. Being part of a community like this adds an extra layer of motivation.&lt;/p&gt;

&lt;p&gt;So far, I’ve spent about 14 hours on the project. Once I finish implementing the database and polish things up a bit, I plan to launch a small web demo. After that, I’ll start promoting it across different platforms hopefully reaching 150 stars on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/shipwrecked/23.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This post documents the state of the AI pdf reader as of May 18, 2025&lt;br /&gt;
To see the current state of the pdf reader visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ai-pdf-reader&quot;&gt;AI pdf reader github&lt;/a&gt;&lt;/p&gt;

</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! Over the past two weeks, I’ve been working hard on the AI PDF Reader. I’ve improved the UI, integrated the AI API, and am now implementing a database to support the reader.&lt;/p&gt;
</description><pubDate>Sun, 18 May 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/improving-the-ai-pdf-reader/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/improving-the-ai-pdf-reader/</guid></item><item>
        <title>Hack Club Shipwrecked</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I wrapped up reviewing the SSD object detection model and was just about to dive into the line-following model—when a message in the Hack Club announcements caught my attention.&lt;/p&gt;

&lt;p&gt;I kicked off the week by discussing the SSD model with Claude, breaking down each component to understand how everything fits together. It really helped me get a better grip on some of the more complex concepts. I also generated a few more inference examples—I’m still amazed at how well the model performs! To top it off, the weights are also getting some attention on Hugging Face too: it’s already been downloaded 90 times this month, and I only published it halfway through.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/model-weight-downloads.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I had just started diving into the line-following model for the ML Rover when a message popped up in the Hack Club announcements channel—and it definitely stole my attention.&lt;/p&gt;

&lt;p&gt;Introducing…
Shipwrecked 🏝️ — a Hack Club hackathon on an island!
From August 8–11, you 🫵 and 130 other Hack Clubbers will gather on Cathleen Stone Island in Massachusetts Bay for a once-in-a-lifetime, 4-day, story-driven hackathon.
As soon as you arrive, you’ll start working together to survive!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To get invited, complete “The Bay”:&lt;/li&gt;
  &lt;li&gt;Spend 60 hours building 4 projects (about 15 hours each) that you’re truly proud of&lt;/li&gt;
  &lt;li&gt;Share them with the world—make at least one go viral (Hack Club will help with workshops, online events, and more)&lt;/li&gt;
  &lt;li&gt;Get invited to Shipwrecked! (Travel stipends available)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As soon as I saw the announcement, a change of plans was inevitable—I decided to postpone the ML Rover project. I mean, how could I pass up the chance to go to Boston for a once-in-a-lifetime Hack Club hackathon on an island?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://shipwrecked.hackclub.com/&quot;&gt;Shipwrecked link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/shipwrecked/shipwrecked.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;So, naturally, I started thinking about what kind of project could actually go viral. I have to admit—I was stuck for a while. Every idea I came up with felt off, and I’d second-guess it almost immediately. That is, until my brother stepped in with an idea he picked up from one of Karpathy’s videos. In it, Karpathy mentioned that he didn’t know of any PDF reader where you could highlight text and chat with an AI about it. And just like that, I knew what I wanted to build.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://shipwrecked.hackclub.com/info/go-viral/&quot;&gt;How to go viral&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should know—I’m not an expert in CSS, JavaScript, or front-end development in general, so I’ve been relying heavily on Claude to help guide me through this project. Still, the core idea is simple but powerful: a normal PDF reader, but enhanced with smart AI features. The main one? You can highlight a passage and instantly get a response from an AI chat that pops up right beside it.&lt;/p&gt;

&lt;p&gt;I started working on it just two days ago, and here’s where it currently stands:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/shipwrecked/open-pdf-landing-page.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/shipwrecked/pdf-reader-home.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Right now, the project has a simple landing page where you can upload and open a PDF. Once the PDF is loaded, you can highlight any text, and a small bubble with a chat icon appears next to your selection. When you click the icon, a chat window opens, showing the highlighted message and giving you the option to choose how you’d like the AI to respond: Simple, Like I’m 5, Technical, or Custom. You can open multiple chats, close them, and ask the AI anything within each one.&lt;/p&gt;

&lt;p&gt;I haven’t integrated the actual AI yet—but that’s coming next week!&lt;/p&gt;

&lt;p&gt;Next week, I’m planning to integrate the AI into the chat feature, fix a few small bugs, and maybe even start exploring some of the other AI features I’ve got in mind. It’s still early, but I’m excited about where this is heading!&lt;/p&gt;

&lt;p&gt;This post documents the state of the AI pdf reader as of April 27, 2025&lt;br /&gt;
To see the current state of the pdf reader visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ai-pdf-reader&quot;&gt;AI pdf reader github&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I wrapped up reviewing the SSD object detection model and was just about to dive into the line-following model—when a message in the Hack Club announcements caught my attention.&lt;/p&gt;
</description><pubDate>Sun, 27 Apr 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/hack-club-shipwrecked/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/hack-club-shipwrecked/</guid></item><item>
        <title>Inference Using SSD Weights</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I took a deeper dive into last week&apos;s model, did some additional training, and let me just say—the inference code made predictions, and they’re genuinely impressive!&lt;/p&gt;

&lt;p&gt;I kicked off the week by listening to a few podcasts created with Notebook LM, which helped set the stage for deeper understanding. I also spent some time revising the code alongside Claude, going through the model architecture and key components more thoroughly. That combination really clicked for me—I came away feeling like I finally had a much clearer grasp of how the model works.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/93cbd2abda91a5a34217186b75c0cade86c84099/ssd/ssd-object-detection.py&quot;&gt;SSD model at this phase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once I felt confident in how the model worked—around Wednesday—I decided it was time to train it. Unfortunately, the initial results were pretty disappointing. Every time I kicked off training, the first few epochs showed a really high loss, which wasn’t too surprising at first. But what really threw me off was that the mAP@0.50 stayed stuck at 0.0000 for the first 10 epochs. Even after that, the improvements were minimal—barely creeping up with tiny values that were barely meaningful.&lt;/p&gt;

&lt;p&gt;After that, I started tweaking some of the weights, hoping to find a sweet spot that would finally unlock the model’s potential. But no matter what I tried, nothing seemed to make a meaningful difference. It was especially frustrating because, on paper, this model had a pretty advanced architecture. It used an EfficientNet‑B1 backbone with an FPN, incorporated sophisticated augmentations through Albumentations, supported mixed‑precision training, and featured a three‑phase learning-rate scheduler. On top of that, the loss function was optimized with GIoU for bounding boxes and Focal Loss for classification—so expectations were high.&lt;/p&gt;

&lt;p&gt;But despite all that, after full training, the model only managed to hit around 40 mAP@0.50—which is disappointingly low for a setup this advanced.&lt;/p&gt;

&lt;p&gt;By this point, I was honestly exhausted. It was Friday, and I’d spent the better part of the week going back and forth, tweaking weights and adjusting parameters, hoping for a breakthrough. But the model just kept underperforming, no matter what I threw at it. So—naturally—I started thinking about giving up.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-0-mAP.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;But—I had one last shot left in me.&lt;/p&gt;

&lt;p&gt;So, I turned to Claude for some insight. Naturally, I was curious—how could a model with such an advanced architecture perform so poorly? I started digging into the original SSD paper, specifically looking at how their model managed to reach 72 mAP@0.5. That’s when it hit me: I had likely overcomplicated my implementation. By layering on too many advanced techniques all at once, I may have introduced more confusion than clarity to the model. Instead of boosting performance, all those additions seemed to interfere with learning.&lt;/p&gt;

&lt;p&gt;I decided to simplify the model, keeping the SSD paper as my main reference. Following its approach step by step, I started stripping away some of the complexity and sticking to the core elements of their design. In the end, the model I ended up with was quite similar to the SSD300 from the paper.&lt;/p&gt;

&lt;p&gt;So, I started training the simplified model. I think the following snippets really say it all.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-training-snippet.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/SSD-training-snippet2.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/SSD-training-snippet3.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The model finished with an mAP@0.5 of 0.76 (76%), which is very close to the 77% reported in the paper. This marked a huge improvement compared to the 40% mAP of the previous model. But now came the moment of truth: I had to test it out. So, I asked Claude to create two simple sets of inference code—one that randomly selects an image from the Pascal VOC dataset and predicts the bounding boxes and labels, and another where you provide the image manually.&lt;/p&gt;

&lt;p&gt;The model did not disappoint.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/1_pred.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/2_pred.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/3_pred.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/4_pred.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/5_pred.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;You can also check out the final trained model weights, now available on Hugging Face. For more examples and the full inference code, head over to the GitHub repo. Feel free to test it, fork it, or build on it!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/pro-grammer/SSD&quot;&gt;SSD weights&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/adrirubio/ml-rover/tree/main/ssd&quot;&gt;SSD github&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next week, I plan to further deepen my understanding by reviewing the newer SSD models and beginning work on the next model for the ML-Rover project. Once that’s complete, I’ll start integrating the models into the Adeept PiCar-B.&lt;/p&gt;

&lt;p&gt;This post documents the state of the SSD model as of April 20, 2025&lt;br /&gt;
To see the current state of the model visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/main/ssd/ssd-object-detection.py&quot;&gt;SSD object detection model&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I took a deeper dive into last week&apos;s model, did some additional training, and let me just say—the inference code made predictions, and they’re genuinely impressive!&lt;/p&gt;
</description><pubDate>Sun, 20 Apr 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/inference-using-SSD/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/inference-using-SSD/</guid></item><item>
        <title>Finalizing SSD Model and Training</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I focused on deepening my understanding of last week’s model by creating inference code and making further improvements. The final results are looking even more promising than before!&lt;/p&gt;

&lt;p&gt;During this week, I spent most of my time reviewing the code and making sure I fully understood some of the harder concepts. Since I’m not creating the model completely from scratch — I use Claude to help take it to the next level — some parts of the code can be harder to grasp (for example, the loss function definition).
To stay organized, I followed some Org mode notes I made a while back:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Tips to improve coding
- Start by learning concepts for new model using coding journal
- Use claude effectively when learning new concepts:
    - Make small modifications to code generated by claude
    - Ask for lots of explanations of concepts
    - Request code outlines to fill in 
    - Ask &quot;why&quot; questions about how the generated code works
- Challenge to write small functions I know how to write:
    - Have claude review the code
- Write in blog weekly to recap week&apos;s work
- Create podcasts for the finished model and listen to further deepen knowledge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To deepen my knowledge of the model, I created some podcasts using Notebook LM (AI-generated podcasts). I would listen to them in the car, at the library, while studying, and more — helping me take my understanding to the next level. By the end of Thursday, I felt that I understood the code much better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-notebook-lm-loading-data.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/SSD-notebook-lm-ssd-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Once I understood the code better, I created some simple inference code for the model (with Claude’s help) to test it out. I didn’t upload the code to GitHub, but running the tests showed that Claude’s predictions from last week were a bit off.&lt;/p&gt;

&lt;p&gt;The real model wasn’t able to detect objects and place bounding boxes with 75–80% mAP as expected. It recognized something, but the placements were often inaccurate, and the classification results had noticeable defects too.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/tree/dbe25b3a8f17d60ce391c47c27b042cb7aa144e1&quot;&gt;SSD model at this phase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, I knew what I had to do.&lt;/p&gt;

&lt;p&gt;The improved SSD model now uses an EfficientNet-B1 backbone with a Feature Pyramid Network (FPN). The training process combines SGD optimization with momentum (0.9) and a three-phase learning rate schedule. I also applied extensive data augmentation techniques, including mosaic transformations, which merge four training images to help improve small object detection.&lt;/p&gt;

&lt;p&gt;For more precise localization, I implemented Generalized IoU (GIoU) loss for bounding box regression. To boost classification accuracy and better handle class imbalance, I used Focal Loss. According to Claude’s predictions, the updated model should achieve 80–84% mAP on the Pascal VOC dataset.&lt;/p&gt;

&lt;p&gt;After making these changes, I created new podcasts with the updated code and plan to start listening to them next week.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-notebook-lm-ssd-architecture2.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/SSD-notebook-lm-ssd-loss.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This post documents the state of the SSD model as of April 13, 2025&lt;br /&gt;
To see the current state of the model visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/main/ssd/ssd-object-detection.py&quot;&gt;SSD object detection model&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I focused on deepening my understanding of last week’s model by creating inference code and making further improvements. The final results are looking even more promising than before!&lt;/p&gt;
</description><pubDate>Sun, 13 Apr 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/finalizing-ssd-model-and-training/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/finalizing-ssd-model-and-training/</guid></item><item>
        <title>Finishing SSD Model and Training</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I completed the model and began the debugging process, which involved making numerous changes across the entire system. After that, I kicked off the first full training run, made some adjustments based on the results, and retrained the model. So far, the results are looking very promising!&lt;/p&gt;

&lt;p&gt;During the debugging phase, the model underwent significant changes. I removed the Pascal VOC dataset that was previously loaded from Hugging Face and instead downloaded the original 2007 Pascal VOC dataset directly to the VM (I’ll explain what the VM is and its role later). I also enhanced the data transformations applied to the datasets by adding augmentations like random shadows, Gaussian noise, and Gaussian blur. These improvements aim to boost training performance and model generalization.&lt;/p&gt;

&lt;p&gt;I adjusted the feature map sizes from [38, 19, 10, 5, 3, 1] to [37, 18, 18, 9, 5, 2] to better balance spatial resolution and receptive fields. Additionally, I modified the VGG backbone: it now uses the full VGG16 feature extractor up to layer 23, then continues from layer 30, aligning more closely with the original SSD architecture. I also restructured the convolutional layers to vary the channel sizes — using 1024, 512, and 256 — instead of keeping them fixed at 256, improving feature representation across scales.&lt;/p&gt;

&lt;p&gt;The loss function remains largely the same, but I introduced checks for empty ground truth boxes and improved the handling of edge cases for increased robustness. The training loop was also enhanced to support early stopping, model checkpointing, and training visualization. Finally, a number of smaller refinements were made throughout the process to get the model fully ready for training.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/tree/6e5d2f788b96d3f1b72b5b5f93db69305c5fc4b1&quot;&gt;SSD model at this phase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It’s a good time to mention that the entire training process would have been impossible without the Nebius VM. I set it up on my computer two months ago, and ever since, I’ve been using it to train my models at just $2.30 an hour. With its H100 GPU, it significantly accelerates training and inference, making it perfect for handling my larger models efficiently.&lt;/p&gt;

&lt;p&gt;And so, I began training.&lt;/p&gt;

&lt;p&gt;The model trained for 35 epochs, finishing with a final loss of 1740.8066 and a best loss of 1648.3888. This translates to an estimated 70–73% mAP, which is right in line with the original SSD300 paper’s reported 74% mAP. Training didn’t take long, and the results were genuinely promising.&lt;/p&gt;

&lt;p&gt;… &lt;br /&gt;
Completed in 0:00:06.431447 &lt;br /&gt;
Epoch 8/35 &lt;br /&gt;
Batch 20/157 - Loss: 3314.0496 &lt;br /&gt;
Batch 40/157 - Loss: 3511.7007 &lt;br /&gt;
Batch 60/157 - Loss: 3111.1152 &lt;br /&gt;
Batch 80/157 - Loss: 2806.2126 &lt;br /&gt;
Batch 100/157 - Loss: 2849.9556 &lt;br /&gt;
Batch 120/157 - Loss: 3000.4382 &lt;br /&gt;
Batch 140/157 - Loss: 2939.6262 &lt;br /&gt;
Batch 157/157 - Loss: 3830.0691 &lt;br /&gt;
Epoch 8 completed in 0:00:06.913375 &lt;br /&gt;
Train Loss: 3302.9789 &lt;br /&gt;
Val Loss: 3030.0992 &lt;br /&gt;
Saving best model with validation loss: 3030.0992 &lt;br /&gt;
… &lt;br /&gt;
Epoch 35/35 &lt;br /&gt;
Batch 20/157 - Loss: 1721.4296 &lt;br /&gt;
Batch 40/157 - Loss: 1765.3861 &lt;br /&gt;
Batch 60/157 - Loss: 1596.6831 &lt;br /&gt;
Batch 80/157 - Loss: 1818.6946 &lt;br /&gt;
Batch 100/157 - Loss: 1464.0607 &lt;br /&gt;
Batch 120/157 - Loss: 1789.6305 &lt;br /&gt;
Batch 140/157 - Loss: 2008.2661 &lt;br /&gt;
Batch 157/157 - Loss: 1806.7076 &lt;br /&gt;
Epoch 35 completed in 0:00:07.030001 &lt;br /&gt;
Train Loss: 1740.8066 &lt;br /&gt;
Val Loss: 1648.3888 &lt;br /&gt;
Final model saved to ssd_pascal_voc_final.pth &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/ssd-loss-plot-old.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;But I wasn’t finished. I knew I could do better.&lt;/p&gt;

&lt;p&gt;So, using insights from the previous results, I made a few key changes. I enabled mixed precision training with AMP to speed up computation and reduce memory usage, and I replaced the standard loss function with Focal Loss to better address class imbalance. I also optimized the learning rate strategy by starting with lower initial values and extended the training duration from 35 to 100 epochs, aiming to improve stability and enhance detection accuracy, especially for harder-to-detect objects.&lt;/p&gt;

&lt;p&gt;The results did not disappoint.&lt;/p&gt;

&lt;p&gt;The model completed its training beautifully! It achieved a final validation loss of 1928.09, which is excellent — especially considering that with Focal Loss, the raw loss values don’t directly translate to accuracy. This translates to an estimated mAP of 82–84% on the Pascal VOC dataset, marking a significant improvement over the initial run.&lt;/p&gt;

&lt;p&gt;Here’s what Claude had to say about the model:&lt;/p&gt;

&lt;p&gt;“Your model has reached a validation loss of 1934.89 at epoch 58, which is even better than I predicted. This loss level puts your SSD model in an excellent position for real-world detection tasks. For context, this level of performance would make your model competitive with many production-ready object detectors.”&lt;/p&gt;

&lt;p&gt;Epoch 1/100 &lt;br /&gt;
Batch 20/157 - Loss: 12212.2188 &lt;br /&gt;
Batch 40/157 - Loss: 13119.7129 &lt;br /&gt;
Batch 60/157 - Loss: 12890.0430 &lt;br /&gt;
Batch 80/157 - Loss: 12896.1777 &lt;br /&gt;
Batch 100/157 - Loss: 10839.2949 &lt;br /&gt;
Batch 120/157 - Loss: 12710.5645 &lt;br /&gt;
Batch 140/157 - Loss: 11867.0479 &lt;br /&gt;
Batch 157/157 - Loss: 10401.2627 &lt;br /&gt;
Warmup phase: learning rate set to 0.000010 &lt;br /&gt;
Epoch 1 completed in 0:00:07.048314 &lt;br /&gt;
… &lt;br /&gt;
Epoch 50/100 &lt;br /&gt;
Batch 20/157 - Loss: 2943.6089 &lt;br /&gt;
Batch 40/157 - Loss: 1767.0057 &lt;br /&gt;
Batch 60/157 - Loss: 2459.3105 &lt;br /&gt;
Batch 80/157 - Loss: 2643.3643 &lt;br /&gt;
Batch 100/157 - Loss: 2142.9924 &lt;br /&gt;
Batch 120/157 - Loss: 2283.0781 &lt;br /&gt;
Batch 140/157 - Loss: 2101.7227 &lt;br /&gt;
Batch 157/157 - Loss: 2728.4829 &lt;br /&gt;
Cosine phase: learning rate set to 0.000011 &lt;br /&gt;
Epoch 50 completed in 0:00:06.342008 &lt;br /&gt;
Train Loss: 2154.6789 &lt;br /&gt;
Val Loss: 1995.7843 &lt;br /&gt;
Saving best model with validation loss: 1995.7843 &lt;br /&gt;
… &lt;br /&gt;
Epoch 100/100 &lt;br /&gt;
Batch 20/157 - Loss: 1751.8605 &lt;br /&gt;
Batch 40/157 - Loss: 2114.5859 &lt;br /&gt;
Batch 60/157 - Loss: 2165.1792 &lt;br /&gt;
Batch 80/157 - Loss: 2106.7981 &lt;br /&gt;
Batch 100/157 - Loss: 2118.8838 &lt;br /&gt;
Batch 120/157 - Loss: 1908.6840 &lt;br /&gt;
Batch 140/157 - Loss: 2345.2627 &lt;br /&gt;
Batch 157/157 - Loss: 1830.1548 &lt;br /&gt;
Cosine phase: learning rate set to 0.000002 &lt;br /&gt;
Epoch 100 completed in 0:00:06.242254 &lt;br /&gt;
Train Loss: 2037.4231 &lt;br /&gt;
Val Loss: 1928.0867 &lt;br /&gt;
Final model saved to ssd_pascal_voc_final.pth &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/ssd-loss-plot-new.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In the next few weeks, my goal is to review all the code to deepen my understanding and ensure everything is well-optimized. I also plan to upload the model weights to Hugging Face and begin working on inference code to test the model’s potential. Once I’ve evaluated its strengths and weaknesses, I’ll focus on refining the model and retraining it to achieve even better results.&lt;/p&gt;

&lt;p&gt;This post documents the state of the SSD model as of April 6, 2025.&lt;br /&gt;
To see the current state of the model visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/main/ssd/ssd-object-detection.py&quot;&gt;SSD object detection model&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello and welcome! This week, I completed the model and began the debugging process, which involved making numerous changes across the entire system. After that, I kicked off the first full training run, made some adjustments based on the results, and retrained the model. So far, the results are looking very promising!&lt;/p&gt;
</description><pubDate>Sun, 06 Apr 2025 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/blog/finishing-model-and-training/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/finishing-model-and-training/</guid></item><item>
        <title>SSD Loss and Training Loop</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! This week, I made significant progress on the SSD object detection model, nearly completing it by implementing the loss function, training loop, loss plotting, and model saving.&lt;/p&gt;

&lt;p&gt;Last week, I completed the model definition. This week, I focused on building the loss function, which calculates the bounding box offset, as well as the training loop which is where the model learns. Additionally, I worked on smaller components such as the optimizer, loss plotting, and model saving.&lt;/p&gt;

&lt;p&gt;First, I instantiated the model we defined last week and began working on the loss function. The box_iou function calculates the Intersection over Union (IoU) between two sets of bounding boxes to determine their overlap. The encode_boxes function converts ground truth box coordinates into a format relative to default anchor boxes, improving the model’s learning process. Finally, the SSD_loss class defines the loss function, combining Smooth L1 loss for localization and Cross-Entropy loss for classification while balancing positive and negative samples for stable training. The loss function’s goal is to evaluate how well the predicted bounding boxes and class probabilities align with the ground truth, guiding the model toward greater accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/box-iou-encoder.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/img/ssd/SSD-loss.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Next, I initialized the SSD loss function and froze the first 10 layers of the VGG backbone to preserve low-level feature extraction, prevent overfitting, and speed up training by focusing updates on the higher layers that learn task-specific features. Additionally, I defined the Adam optimizer with a lower learning rate for the first and second convolution layers of the backbone to ensure more stable and gradual updates in the early feature extraction stages.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-freezing-layers-and-optimizer.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now, it was time for the actual training loop. The loop processes batches of images, computes the loss using the SSDLoss function, backpropagates gradients, and updates the model parameters using the optimizer. After each epoch, it evaluates the model on the validation set without updating weights, records the average training and validation losses, and prints the epoch’s performance metrics. The goal is to optimize the model’s parameters by minimizing training loss while tracking validation performance to ensure generalization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-training-loop.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Finally, I added basic loss plotting to visualize the learning curve, along with code to save the trained model. I also conducted several debugging sessions to refine and improve various aspects of the code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/SSD-loss-plotting-and-model-saving.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This post documents the state of the SSD model as of March 30, 2025.&lt;br /&gt;
To see the current state of the model visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/main/ssd/ssd-object-detection.py&quot;&gt;SSD object detection model&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! This week, I made significant progress on the SSD object detection model, nearly completing it by implementing the loss function, training loop, loss plotting, and model saving.&lt;/p&gt;
</description><pubDate>Sun, 30 Mar 2025 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/SSD-loss-and-training-loop/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/SSD-loss-and-training-loop/</guid></item><item>
        <title>SSD Object Detection</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! In this post, I&apos;ll explain the SSD architecture for object detection, including an overview of the model&apos;s structure and which parts have already been implemented.&lt;/p&gt;

&lt;p&gt;So, what is SSD?&lt;/p&gt;

&lt;p&gt;SSD (Single Shot MultiBox Detector) is an object detection architecture that divides an image into a grid and predicts bounding boxes and class probabilities directly from feature maps at multiple scales. It leverages convolutional layers to extract features and applies predefined anchor boxes of various sizes and aspect ratios at each location, allowing it to efficiently detect objects of different scales. Unlike two-stage detectors, SSD performs detection in a single network pass, making it significantly faster while maintaining competitive accuracy—ideal for real-time applications.
That might sound like a lot to take in, but by the end of this post, you’ll have a clearer understanding.&lt;/p&gt;

&lt;p&gt;Now, here’s what my model’s structure will look like:&lt;/p&gt;

&lt;p&gt;-Model: SSD300&lt;br /&gt;
-Base Network: VGG16&lt;br /&gt;
-Input Resolution: 300×300&lt;br /&gt;
-Feature Maps: 6 scales for multi-scale detection&lt;br /&gt;
-Default Boxes: ~8000 per image&lt;br /&gt;
-Output: Class scores and box offsets for each default box&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;First, I began by loading the dataset for training and validation. For the SSD model, I chose the PASCAL VOC dataset. After that, I applied the necessary transformations and converted the dataset into PyTorch format to ensure compatibility with the training pipeline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/pascal-voc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, I created a collate function and set up the data loaders.&lt;br /&gt;
After that, I began working on the actual model definition.&lt;/p&gt;

&lt;p&gt;The first step was to load the first and second convolutional layers from a pretrained PASCAL VOC model. This is done to allow the network to leverage learned features, speeding up training and improving generalization. Then, I added four custom convolutional layers to further refine feature extraction for object detection. The convolutional layers have sizes of 38×38, 19×19, 10×10, 5×5, 3×3, and 1×1. Each layer produces a feature map, capturing details from fine-grained features at 38×38 to high-level, larger features at 1×1, enabling the model to detect objects at multiple scales.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/conv-layers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once a feature map was generated for each convolutional layer, the next step was to add random default anchor boxes at every index of the feature maps. I defined anchor box configurations for each feature map and then created a function with a nested loop. This function iterates through every index in the feature map, traversing its rows and columns to locate the center of each grid cell. It places default boxes of various sizes and aspect ratios surrounding the the center, ensuring robust multi-scale object detection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/default-boxes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, we are ready to predict the bounding box adjustments and class probabilities for the input image. This step is often referred to as the “prediction head”. Within the SSD model, I defined a sequence of convolutional layers responsible for predicting the bounding box offsets and class probabilities for each default box.&lt;/p&gt;

&lt;p&gt;In the forward pass, the input image is passed through these convolutional layers, which produce feature maps at different scales. These feature maps are then used to generate predictions for each default box. For each default box, the network predicts two things: the offsets (or adjustments) for the bounding box coordinates and the class probabilities for the object within the box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ssd/forward-function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This post documents the state of the SSD model as of March 23, 2025.&lt;br /&gt;
To see the current state of the model visit:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/adrirubio/ml-rover/blob/main/ssd/ssd-object-detection.py&quot;&gt;SSD object detection model&lt;/a&gt;&lt;/p&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! In this post, I&apos;ll explain the SSD architecture for object detection, including an overview of the model&apos;s structure and which parts have already been implemented.&lt;/p&gt;
</description><pubDate>Sun, 23 Mar 2025 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/SSD-object-detection/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/SSD-object-detection/</guid></item><item>
        <title>Starting CNNs</title>
        <description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! This week, I wrapped up work on the blog and began working on the ML-Rover project.&lt;/p&gt;

&lt;p&gt;I started the week by refining the blog, adding dark mode, and making a few minor improvements. For now, the blog is complete, except for the weekly post summarizing my coding progress.&lt;/p&gt;

&lt;p&gt;Dark mode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dark-mode.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I felt the need to refresh my understanding of CNN concepts, so I watched a couple of videos on the topic and revisited the ImageNet Classification with Deep Convolutional Neural Networks paper by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.&lt;/p&gt;

&lt;p&gt;Now seems like a good time to mention that I use the Cursor editor—the AI integration is a really cool feature!&lt;/p&gt;

&lt;p&gt;For the ML-Rover, my goal is to create multiple AI models, each serving a specific purpose, to create a fully integrated and advanced AI system for the rover.&lt;/p&gt;

&lt;p&gt;Here are some of the models I plan to build:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;computer vision projects –&amp;gt; object detection&lt;/li&gt;
  &lt;li&gt;driving behavior cloning&lt;/li&gt;
  &lt;li&gt;autonomous navigation with deep reinforcement learning&lt;/li&gt;
  &lt;li&gt;visual SLAM (simultaneous localization and mapping)&lt;/li&gt;
  &lt;li&gt;voice command navigation&lt;/li&gt;
  &lt;li&gt;dynamic path planning&lt;/li&gt;
  &lt;li&gt;imitation learning with style transfer –&amp;gt; aggressive, cautious, smooth&lt;/li&gt;
&lt;/ul&gt;
</description><description>&lt;p class=&quot;intro&quot;&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ello, and welcome! This week, I wrapped up work on the blog and began working on the ML-Rover project.&lt;/p&gt;
</description><pubDate>Sun, 16 Mar 2025 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/blog/starting-with-CNNs/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/starting-with-CNNs/</guid></item></channel>
</rss>
